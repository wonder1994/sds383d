\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (chap number)
% counter and make various numbering schemes work relative
% to the chap number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\chap}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf SDS 383D: Modeling II
	\hfill Spring 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill  Section #1 #2  \hfill} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{ Section #1 #2}{ Section #1 #2}

   
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
%\renewcommand{\cite}[1]{[#1]}
%\def\beginrefs{\begin{list}%
%        {[\arabic{equation}]}{\usecounter{equation}
%         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
%         \setlength{\labelwidth}{1.6truecm}}}
%\def\endrefs{\end{list}}
%\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{exercise}{Exercise}[lecnum]
\newtheorem{example}{Example}[lecnum]
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\newcommand\Prob{\mathbf{P}}
\newcommand\Q{\mathbf{Q}}
\newcommand\cov{\mbox{cov}}
\begin{document}
\chap{1}{(Solutions by Xinjie Fan)}
\maketitle

\section{Exchangeability and de Finetti's theorem}

\begin{exercise}Clearly, all iid sequences are exchangeable, but not all exchangeable sequences are iid. Consider an urn, containing r red balls and b blue balls. A sequence of colors is generated by repeatedly sampling a ball from the urn, noting its color, and then returning the ball, plus another ball of the same color, to the urn. Show that the resulting sequence is exchangeable, but not iid.
\end{exercise}

\begin{proof}
The sequence is obviously not iid. We only show the sequence is exchangeable in the following.\\
Consider a sequence of $N$ colors, $(X_1, ..., X_N)$, $$\Prob(X_1, ..., X_N) = \Prob(X_1)\Prob(X_2|X_1)\cdots\Prob(X_N|X_1,..., X_{N-1}),$$
where
\begin{equation}
\Prob(X_i|X_1,...,X_{i-1}) =
\begin{cases}
\frac{r+j-1}{r+b+i-1} & \textrm{  $X_i$ is the $j$th red ball in the sequence}\\
\frac{b+j-1}{r+b+i-1} & \textrm{ $X_i$ is the $j$th blue ball in the sequence}.
\end{cases}\nonumber
\end{equation}
Notice that the dominator does not depend on $X_1, ..., X_N$. Therefore, neither does the dominator of the joint distribution. We only need to show that the numerator of the joint distribution does not depend on the order of $X_1, ..., X_N.$ If we consider the product of numerators corresponding to red balls first, it would be $r(r+1)\cdots(r+m-1)$, where $m$ is the number of red balls in $X_1,...,X_N$. Similarly, the product of numerators corresponding to blue balls would be $b(b+1)\cdots(b+n-1)$, where $n$ is the number of blue balls in $X_1,...,X_N$. Then the numerator of the joint distribution is $r(r+1)\cdots(r+m-1)b(b+1)\cdots(b+n-1)$ which does not depend on the order of $X_1, ..., X_N$. Therefore, they are exchangeable.


\end{proof}


\begin{exercise}
  We will start off with a finite sequence $(X_1,\dots, X_M)$. For any $N\leq M$, show that
  $$\Prob\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) = \frac{{t\choose s}{M-t\choose N-s}}{{M \choose N}}$$
\end{exercise}

\begin{proof}
Our assumption is that $(X_1, ..., X_M)$ is exchangeable.\\
Claim:
\begin{equation}
\Prob\left(X_1, .., X_M| \sum_{i=1}^M X_i = t\right)=
\begin{cases}
1/ {M \choose t}& \sum_{i=1}^M X_i = t\\
0 & \sum_{i=1}^M X_i \neq t
\end{cases}\nonumber
\end{equation}
The reason is condition on $\sum_{i=1}^M X_i = t$, there are ${M \choose t}$ different sequences, and one sequence can be transformed to another by rearranging the order. Therefore, they share the same probability (exchangeable) and sum up to one.


Therefore, we have the following(the second equation is a simple combinatorial step, counting how many different sequences has $s$ ones in the first $N$ numbers; the third equation is easy to check by expanding both sides):
\begin{equation}
\begin{split}
\Prob\left(\sum_{i=1}^N X_i = s\Big|\sum_{i=1}^M X_i = t\right) &= \sum_{\sum_{i=1}^N X_i = s, \sum_{i=1}^M X_i = t} \Prob\left(X_1, .., X_M| \sum_{i=1}^M X_i = t\right)\\
& = \frac{{N\choose s}{M-N\choose t-s}}{{M \choose t}} = \frac{{t\choose s}{M-t\choose N-s}}{{M \choose N}}
\end{split}\nonumber
\end{equation}


\end{proof}








We can therefore write
\begin{equation}\Prob\left(\sum_{i=1}^N X_i = s\right) = {N\choose s}\sum_{t=s}^{M-N+s}\frac{(t)_s(M-t)_{n-s}}{(M)_N}\Prob\left(\sum_{i=1}^M X_i = t\right),\label{eqn:a}\end{equation}
where $(x)_y = x(x-1)\dots (x-y+1)$.

Let $F_M(\theta)$ be the distribution function of $\frac{1}{M}(X_1, + \dots, + X_M)$  --  i.e.\ a step function between 0 and 1, with steps of size $\Prob(\sum_i X_i= t)$ at $t=0,1,\dots, M$. Then we can rewrite Equation~\ref{eqn:a} as

$$\Prob\left(\sum_{i=1}^N X_i = s\right) = {N \choose s}\int_0^1\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N}dF_M(\theta)$$
\begin{exercise}
  Show that, as $M\rightarrow \infty$, we can write
  $$\Prob\left(\sum_{i=1}^N X_i = s\right) \rightarrow {N \choose s}\int_0^1\theta^s(1-\theta)^{N-s}dF_M(\theta)$$
\end{exercise}

\begin{proof}
Loosely speaking, as $M \rightarrow \infty$, $(M\theta)_s \rightarrow (M\theta)^s$, and similarly $(M(1-\theta))_{N-s} \rightarrow (M(1-\theta))^{N-s}$, $(M)_N \rightarrow M^N$. Therefore, $\frac{(M\theta)_s(M(1-\theta))_{N-s}}{(M)_N} \rightarrow\theta^s(1-\theta)^{N-s},$
\end{proof}

The proof is completed using a result (the Helly Theorem), that shows that any sequence $\{F_M(\theta); M=1,2,\dot\}$ of probability distributions on [0,1] contains a subsequence that converges to $F(\theta)$.


\section{The exponential family of distributions}

\begin{exercise}
  The Poisson random variable has PDF
  $$p(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}$$
  Re-write the density of the Poisson random variable in exponential family form. What are $\eta$, $T(x)$, $A(\eta)$ and $h(x)$? What about if we have $n$ independent samples $x_1,\dots, x_n$?
\end{exercise}

\begin{proof}
$$\Prob(x|\lambda) = \frac {1}{x!}\exp (\log(\lambda)x - \lambda);$$
$$\Prob(x_1,..., x_n|\lambda) = \frac{1}{\prod_{i=1}^nx_i!}\exp(\log(\lambda)(\sum_{i=1}^nx_i)-n\lambda).$$
\end{proof}



\begin{exercise}
  The gamma random variable has PDF
  $$p(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$$
  What are the natural parameters and sufficient statistics for the gamma distribution, given $n$ observations $x_1,\dots, x_N$?
\end{exercise}
\begin{proof}
  $$p(x|\alpha,\beta) =\exp\left( (-\beta, \alpha-1){x \choose \log x} - (\log(\Gamma(\alpha))-\alpha \log\beta)\right).$$
  $$p(x_1, ..., x_n|\alpha,\beta) =\exp\left( (-\beta, \alpha-1){\sum x_i \choose \sum \log x_i} - n(\log(\Gamma(\alpha))-\alpha \log\beta)\right).$$
  The natural parameters are $(-\beta, \alpha-1)$ and sufficient statistics are $(\sum x_i, \sum \log x_i)$.
\end{proof}



\begin{exercise}
  For exponential family random variables, we know that the sufficient statistic $T(X)$ contains all the information about $X$, so (for univariate $X$) we can write the moment generating function of the sufficient statistic as $\E[e^{sT(x)}|\eta]$. Show that the moment generating function for the sufficient statistic of  an arbitrary exponential family random variable with natural parameter $\eta$ can be written as
    $$M_{T(X)}(s) = \exp\{A(\eta+s) - A(\eta)\}$$
\end{exercise}

\begin{proof}
\begin{equation}
\begin{split}
M_{T(X)}(s) =E[\exp(s^TT(X))]& = \int \exp\{s^TT(X)\}h(X)\exp\{\eta^TT(X)-A(\eta)\}dX\\
& = \int h(X)\exp\{(s+\eta)^TT(X)-A(\eta+s)\}dX\exp\{A(\eta+s) - A(\eta)\}\\
 &=\exp\{A(\eta+s) - A(\eta)\}
\end{split}
\end{equation}
\end{proof}

\begin{exercise}
  It is usually easier to calculate mean and variance using the cumulant generating function rather than the moment generating function. Starting from the exponential family representation of the Poisson distribution from Exercise 1.4, calculate the mean and variance of the Poisson using a) the moment generating function, and b) the cumulant generating function.
  \end{exercise}
\begin{proof}
$$E[e^{sx}] = e^{-\lambda}\int_0^\infty \frac{e^{sx}\lambda^x }{x!}dx = e^{-\lambda}\int_0^\infty \frac{(e^s\lambda)^x }{x!}dx = \frac{e^{e^s\lambda}}{e^\lambda}$$

(a) Moment generating function:
$$\frac{dE[e^{sX}]}{ds} = \lambda e^{\lambda e^s -\lambda +s};\quad \frac{d^2E[e^{sX}]}{ds^2} = \lambda e^{\lambda e^s -\lambda +s}(\lambda e^s +1 ).$$
Therefore, $E[X] = \frac{dE[e^{sX}]}{ds}\Big|_{s=0} = \lambda$, $E[X^2] = \frac{d^2E[e^{sX}]}{ds^2}\Big|_{s=0} = \lambda(\lambda+1)$, and $var(X) = \lambda(\lambda+1)-\lambda^2 = \lambda$.\\
(b) Cumulant generating function:
$$C_X(s) = (e^s-1)\lambda; \quad \frac{dC_X(s)}{ds} = \lambda e^s;\quad \frac{d^2C_X(s)}{ds^2} = \lambda e^s.$$
Therefore, $E[X] = \frac{dC_X(s)}{ds}\Big|_{s=0} = \lambda$, $var(X) = \frac{d^2C_X(s)}{ds^2}\Big|_{s=0} = \lambda$.
\end{proof}

\begin{exercise}
  Suppose we have $N$ independent observations $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$. If $\sigma^2$ is known and $\mu\sim \mbox{Normal}(\mu_0,\sigma_0^2)$, derive the posterior for $\mu|x_1,\dots, x_N$
\end{exercise}

\begin{proof}
\begin{equation}
\begin{split}
p(\mu |x_1, ..., x_N) & \propto \exp\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\}\prod_{i=1}^N\exp\{-\frac{(x_i-\mu)^2}{2\sigma^2}\}\\
& \propto \exp\{-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}\}\exp\{-\frac{n\mu^2-2\sum_{i=1}^N x_i\mu}{2\sigma^2}\}\\
& \propto \exp\{-\frac{(n/\sigma^2+1/\sigma_0^2)\mu^2 - 2(\mu_0/\sigma_0^2+\sum x_i/\sigma^2))\mu}{2}\}.
\end{split}
\end{equation}
Therefore, $\mu|x_1,..., x_N\sim N(\frac{\mu_0/\sigma_0^2+\sum x_i/\sigma^2}{n/\sigma^2+1/\sigma_0^2}, \frac{1}{n/\sigma^2+1/\sigma_0^2})$
\end{proof}

\begin{exercise}
  Now, let's assume  $x_1,\dots, x_N \stackrel{\small{iid}}{\sim}\mbox{Normal}(\mu, \sigma^2)$ with known mean $\mu$ but unknown variance $\sigma^2$. Let's express the likelihood in terms of the precision, $\omega=1\sigma^2$:
  $$f(x_i|\mu, \omega) = \sqrt{\frac{\omega}{2\pi}} \exp\left\{-\frac{\omega}{2}(x_i-\mu)^2\right\}$$
  Let $\omega$ have a gamma prior (this is also known as putting an inverse-gamma prior on $\sigma^2$):
  $$p(\omega) = \frac{\beta^\alpha}{\Gamma(\alpha)}\omega^{\alpha-1}e^{-\beta \omega}$$
  Derive the posterior distribution for $\omega$.
\end{exercise}

\begin{proof}
\begin{equation}
\begin{split}
p(\omega |x_1, ..., x_N) & \propto\omega^{\alpha-1}e^{-\beta\omega}\prod_{i=1}^N\omega^{1/2}\exp\{-\frac{\omega(x_i-\mu)^2}{2}\}\\
& \propto \omega^{n/2+\alpha-1}\exp\{-(\beta+\sum(x_i-\mu)^2/2)\omega\}.
\end{split}
\end{equation}
Therefore, $\omega | x_1, ..., x_N \sim Gamma(n/2 + \alpha, \beta +\sum(x_i-\mu)^2/2)$.
\end{proof}
\begin{exercise}
  Let's assume $x \sim \mbox{Normal}(0, \sigma^2)$ and that $\sigma^2\sim\mbox{InvGamma}(\alpha,\beta)$ (i.e.\ $1/\sigma^2 \sim \mbox{Gamma}(\alpha,\beta)$). Show that the marginal distribution of $x$ is given by a Student's $t$ distribution.
\end{exercise}

\begin{proof}
\begin{equation}
\begin{split}
f(x) &= \int f(x|\sigma^2)f(\sigma^2)d\sigma^2\\
& =\frac{\beta^\alpha}{\Gamma(\alpha)\sqrt{2\pi}} \int _0^\infty (\frac{1}{\sigma^2})^{\alpha + 3/2}e^{-(\frac{x^2/2+ \beta}{\sigma^2})}d \sigma^2\\
& = \frac{\beta^\alpha\Gamma(\alpha +1/2)}{\Gamma(\alpha)\sqrt{2\pi}(x^2/2+ \beta)^{\alpha +1/2}}
\end{split}
\end{equation}
Therefore, $x \sim t(\nu=2\alpha, \sigma = \sqrt{\beta/\alpha})$.
\end{proof}



\section{Multivariate normal distribution}

\begin{exercise}[covariance matrix]
  The covariance matrix $\Sigma$ of a vector-valued random variable $x$ is the matrix whose entries $\Sigma(i,j) = \cov(x_i,x_j)$ are given by the covariance between the $i$th and $j$th elements of $x$, giving

  $$\Sigma = \E\left[(x-\mu)(x-\mu)^T\right]$$

  Show that a) $\Sigma = \E[xx^T] - \mu\mu^T$; b) if the covariance of $x$ is $\sigma$, then the covariance of $Ax+b$ is $A\Sigma A^T$
\end{exercise}

\begin{proof}
(a)$$\Sigma = \E\left[(x-\mu)(x-\mu)^T\right] = \E\left[xx^T-2x\mu^T+\mu\mu^T\right] =\E\left[xx^T\right] -2\E\left[x\right] \mu^T+ \mu\mu^T =  \E[xx^T] - \mu\mu^T$$
(b)
\begin{equation}
\begin{split}
Cov(Ax+b) &= \E\left[(Ax+b-\E(Ax+b))(Ax+b-\E(Ax+b))^T\right] \\
&= \E\left[(Ax-\E(Ax))(Ax-\E(Ax))^T\right] = Cov(Ax) \\
&= \E[(Ax)(Ax)^T] - \E[Ax]\E[Ax]^T\\
& = A\{\E[xx^T]-\E[x]\E[x]^T\}A^T\\
& = ACov(x)A^T
\end{split}
\end{equation}

\end{proof}

\begin{exercise}[Standard multivariate normal]
  The simplest multivariate normal, known as the standard multivariate normal, occurs where the entries of $x$ are independent and have mean 0 and variance 1. a) What is the moment generating function of a univariate normal, with mean $m$ and variance $v^2$? b) Express the PDF and moment generating function of the standard multivariate normal, in vector notation.
\end{exercise}

\begin{proof}
(a)Suppose $X\sim {\cal N}(m, v^2)$, then
\begin{equation}
\begin{split}\label{mgf}
\E[e^{tX}] &= \int e^{tx}\frac{1}{\sqrt{2\pi v^2}}e^{-\frac{(x-m)^2}{2v^2}}dx\\
&= e^{\frac{(m+v^2t)^2-m^2}{2v^2}}\frac{1}{\sqrt{2\pi v^2}}\int e^{-\frac{(x-(m+v^2t))^2}{2v^2}}dx\\
&= e^{\frac{(m+v^2t)^2-m^2}{2v^2}} = e^{\frac{2mt+v^2t^2}{2}}
\end{split}
\end{equation}

(b)
Suppose $X\sim {\cal N}(0, I),$ then 
$$p(x) = \frac{1}{|det(2\pi I)|^{\frac 12}}\exp \{-\frac{X^TX}{2}\}.$$
The moment generating function can be calculated as follows:
\begin{equation}
\begin{split}
\E[e^{tX}] &=  \exp\{\frac{t^Tt}{2}\}\frac{1}{|det(2\pi I)|^{\frac 12}}\int \exp \{-\frac{(X-t)^T(X-t)}{2}\}dX\\
&= \exp\{\frac{t^Tt}{2}\}
\end{split}
\end{equation}


\end{proof}

\begin{exercise}[Multivariate normal]
  A random vector $x$ has multivariate normal distribution if and only if every linear combination of its elements is univariate normal, i.e.\ if the scalar value $z = a^Tx$ is normally distributed for all possible $x$. Prove that this implies that $x$ is multivariate normal if and only if its moment generating function takes the form $M_X(s) = \exp\{s^T\mu + \frac 12 s^T\Sigma s\}$. Then, for any $a$any linear, where $\mu$ and $\Sigma$ are the mean and covariance of $x$. \textit{Hint: We know the moment generating function of $z$ in terms of the mean and variance of $z$, from the previous question...}
\end{exercise}
    
\begin{proof}
(a) First, we assume that $X$ is a multivariate normal. Then $s^TX$ is distributed as ${\cal N}(s^T\mu, s^T\Sigma s)$, so using equation \ref{mgf} with $t = 1$, we have $M_X(s) = \E(e^{s^TX}) = e^{\frac{2s^T\mu+s^T\Sigma s}{2}}$.\\
(b) Now, we assume that the moment generating function takes the form $M_X(s) = \exp\{s^T\mu + \frac 12 s^T\Sigma s\}$. Then, for any $a$, consider the moment generating function of $a^TX$:
$$E[e^{sa^TX}] = \exp\{sa^T\mu+sa^T\Sigma as\} = \exp\{sa^T\mu+a^T\Sigma as^2\}.$$

Therefore, $a^TX$ is normally distributed for any $a$.

\end{proof}

\begin{exercise}[Relationship to standard multivariate normal]
  An equivalent statement is that a random vector $x$ has multivariate normal distribution if and only if it can be written in the form
  $$x = Dz + \mu$$
  for some matrix $D$, real-valued vector $\mu$, and vector $z$ distributed according to a standard multivariate normal. Express the moment generating function of $x$ in terms of $D$, and uncover the relationship between $D$ and $\Sigma$. Use this result to suggest a method for generating multivariate normal random variables, if you have a method for generating Normal(0,1) univariate random variables.
\end{exercise}

\begin{proof}
$$\E[e^{s^Tx}] = \E[e^{s^TDx+s^T\mu}] = e^{s^T\mu}E[e^{s^TDz}] = e^{s^T\mu}M_z(s^TD).$$
Since $\Sigma = Cov(X) = Cov(Dz+\mu) = DD^T$, $\Sigma = DD^T$. To generate an arbitrary normal distribution, we can first generate samples standard normal distribution and then transform the samples with the linear transformation.
\end{proof}

\begin{exercise}
  Use the result from the previous question to show that the PDF of a multivarite normal random vector $x\sim\mbox{Normal}(\mu, \Sigma)$ takes the form

  $$p(x) = \frac{1}{(2\pi)^{n/2}}|\Sigma|^{-1/2}\exp\left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\},$$
by using a change-of-variables from the standard multivariate normal distribution.
\end{exercise}
\begin{proof}
$$p(x) = p(z)/|det(\frac{\partial x}{\partial z})| = \frac{1}{|det(D)(2\pi)^{n/2}|} \exp\left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\}.$$

Since $DD' = \Sigma$, $det(D)$ is equal to $ \sqrt{det(\Sigma)},$ which completes the proof.
\end{proof}

\subsection{Manipulation of multivariate normals}
\begin{exercise}[marginal distribution]
  Let us assume that $x\sim \mbox{Normal}(\mu, \Sigma)$, and let us partition $x$ into 2 components $x_1$ and $x_2$. Let us similarly partition $\mu$ and $\sigma$ so that

  $$\mu  = (\mu_1, \mu_2)^T \qquad \qquad \Sigma = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}\end{pmatrix} = \begin{pmatrix}\Sigma_{11} & \Sigma_{12} \\ \Sigma_{12}^T & \Sigma_{22}\end{pmatrix}$$

  Derive the marginal distribution of $x_1$.
\end{exercise}

\begin{proof}
Note that $x_1 = (I, 0)x$, so $x_1$ is a linear transformation of $x$ and therefore normally distributed. 

$\E [x_1] = (I, 0)\mu = \mu_1,$
$Cov(x_1) = (I,0)\Sigma(I,0)^T = \Sigma_{11}.$
Therefore, $x_1 \sim {\cal N} (\mu_1, \Sigma_{11}).$
\end{proof}
\begin{exercise}[Precision matrix]
  Earlier, we chose to express a univariate normal random variable in terms of its precision, to make math easier. We can also express a multivariate normal in terms of a precision matrix $\Omega = \Sigma^{-1}$. Partition $\Omega$ as
  $$\Omega = \begin{pmatrix}\Omega_{11} & \Omega_{12} \\ \Omega_{12}^T & \Omega_{22}\end{pmatrix}$$
  and express $\Omega_{11}$, $\Omega_{12}$ and $\Omega_{22}$ in terms of $\Sigma_{11}$, $\Sigma_{12}$ and $\Sigma_{22}$. \textit{Hint: You'll need the matrix inversion lemma}
\end{exercise}
\begin{proof}
With block matrix inversion formula, we have 
$$\Omega_{11} = \Sigma_{11}^{-1}+\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}\Sigma_{21}\Sigma_{11}^{-1}$$
$$\Omega_{12} = -\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{21})^{-1}$$
$$\Omega_{22} = (\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}$$

\end{proof}
\begin{exercise}[Conditional distribution]
  The conditional distribution of $x_1|x_2$ is also normal, with mean $\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2)$ and covariance $\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T$. Prove this for the case where $\mu$ is zero (the general case isn't really harder, just more tedious). \textit{Hint: ignore any constants that don't involve $x_1$. You might want to work with the log conditional density.}
\end{exercise}
\begin{proof}
Consider the random vector $(x_1- \Sigma_{12}\Sigma_{22}^{-1}x_2,x_2)$. Since it is a linear transformation of a normal random vector, so itself is normally distributed as well. Observe that $Cov(x_1-\Sigma_{12}\Sigma_{22}^{-1}x_2,x_2) = Cov(x_1, x_2) - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22} = 0.$ The zero correlation implies independence within normally distributed random vectors. 

Therefore, $x_1- \Sigma_{12}\Sigma_{22}^{-1}x_2|x_2$ has the same distribution as $x_1- \Sigma_{12}\Sigma_{22}^{-1}x_2$, namely $x_1- \Sigma_{12}\Sigma_{22}^{-1}x_2|x_2 \sim {\cal N}(\mu_1-\Sigma_{12}\Sigma_{22}^{-1}\mu_2, \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})$. Therefore, $x_1|x_2\sim {\cal N}(\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2), \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^T) $

\end{proof}
%\begin{exercise}[Conjugacy]
%  If $x\sim \mbox{Normal}(\mu, \Sigma)$ and $\mu \sim \mbox{Normal}(\mu_0,\Sigma_0)$, derive the posterior of $\mu|x$ \textit{again, ignoring normalizing constancts will make this easier}.
%\end{exercise}
  

\section{Frequentist estimation and uncertainty quantification}

\begin{exercise}[method of moments]
  To obtain the theoretical moments, we can assume that $E[y_i|x_i] = x_i^T\beta$, implying that the covariance between the predictors $x_i$ and the residuals is zero. By setting the sample covariance between the $x_i$ and the $\epsilon_i$ to zero, derive a method of moments estimator $\hat{\beta}_{MM}$
\end{exercise}
\begin{proof}
\begin{equation}
\begin{split}
Cov(y-\beta^Tx, x) &= \E[(y-\beta^Tx)x^T]-\E[y-\beta^Tx]\E[x^T]\\
& = \frac{\sum_iy_ix_i^T - \beta^Tx_ix_i^T}{N}-\frac{\sum_iy_i-\beta^Tx_i}{N}\frac{\sum_ix_i^T}{N}\\
& = \frac{\sum_iy_ix_i}{N}-\bar{y}\bar{x} - \beta^T(\frac{\sum x_ix_i^T}{N}-\bar{x}\bar{x}^T),
\end{split}
\end{equation}
where $\bar{y} = \sum_i y_i/N$, and $\bar{x} = \sum_i x_i/N.$
Set $Cov(y-\beta^Tx,x)$ to zero, we have $\hat{\beta}_{MM}^T= (\frac{\sum_iy_ix_i}{N}-\bar{y}\bar{x})(\frac{\sum x_ix_i^T}{N}-\bar{x}\bar{x}^T)^{-1}.$ And furthermore, if we assume that $\E(\epsilon) = 0$, we have $\hat{\beta}_{MM}^T= ({\sum_iy_ix_i})({\sum x_ix_i^T})^{-1}.$
\end{proof}
\begin{exercise}[maximum likelihood]
  Show that, if we assume $\epsilon_i\sim \mbox{Normal}(0,\sigma^2)$, then the ML estimator $\hat{\beta}_{ML}$ is equivalent to the method of moments estimator.
\end{exercise}

\begin{proof}
\begin{equation}loglikelihood = -\sum_i \frac{(y_i-\beta^Tx_i)^2}{2\sigma^2} + constant\label{likelihood}
\end{equation}
Take the derivative with respect to $\beta$ and set it to $0$, we have $\sum_i(y_i-\beta^Tx_i)x_i^T = 0$, which is solved by $\hat{\beta}_{ML}^T = (\sum_i y_ix_i^T)(\sum_i x_ix_i^T)^{-1}$
\end{proof}

\begin{exercise}[Least squares loss function]
  Show that if we assume a quadratic loss function, i.e.\ $\hat{\beta}_{LS} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2$, we recover the same estimator again.
\end{exercise}

\begin{proof}
As is shown in equation \ref{likelihood}, maximizing loglikelihood is equivalent to minimizing $\sum_{i=1}^N(y_i - x_i^T\beta)^2$
\end{proof}
\begin{exercise}[Ridge regression]
  We may wish to add a regularization term to our loss term. For example, ridge regression involves adding an L2 penalty term, so that
  $$\hat{\beta}_{\small{ridge}} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2 \; s.t. \; \sum_{j=1}^p \beta_j^2 \leq t$$
  for some $t\geq 1$.

  Reformulate this constrained optimization using a Lagrange multiplier, and solve to give an expression for $\hat{\beta}_{\small{ridge}}$. Comparing this with the least squares estimator, comment on why this estimator might be prefered in practice.
\end{exercise}
\begin{proof}
Unconstrained version of ridge regression:
$$\hat{\beta}_{\small{ridge}} = \arg\min_{\beta\in \mathbb{R}^p}\sum_{i=1}^N(y_i - x_i^T\beta)^2 + \lambda\sum_{j=1}^p (\beta_j^2-t).$$

Take the derivative with respect to $\beta$ and set it to $0$, we have $-2\sum_i x_i(y_i-x_i^T\beta) + 2\lambda \beta = 0$, which is solved by $\hat{\beta}_{ridge} = (\sum_ix_ix_i^T+ \lambda I)^{-1}(\sum_ix_iy_i)$
\end{proof}
  
\subsection{Uncertainty quantification}

\begin{exercise}
  What is the sampling distribution for $\hat{\beta}_{LS}$ ($=\hat{\beta}_{MM}=\hat{\beta}_{ML}$)?
\end{exercise}
\begin{proof}
Since $\hat{\beta}_{LS} = (X^TX)^{-1}X^Ty$, which is a linear transformation of a normal random vector $y$, so itself is a normal random vector as well.
$\E(\hat{\beta}_{LS}) = \beta$, and $Cov(\hat{\beta}_{LS}) = \sigma^2(X^TX)^{-1}.$
Therefore, $\hat{\beta}_{LS}\sim {\cal N}( \beta,\sigma^2(X^TX)^{-1}).$
\end{proof}
\begin{exercise}
  How about the sampling distribution for $\hat{\beta}_{\small{ridge}}$?
\end{exercise}
\begin{proof}
Since $\hat{\beta}_{ridge} = (X^TX+\lambda I)^{-1}X^Ty$, which is a linear transformation of a normal random vector $y$, so itself is a normal random vector as well.
$\E(\hat{\beta}_{ridge}) = (X^TX+\lambda I)^{-1}X^TX\beta$, and $Cov(\hat{\beta}_{ridge}) = \sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}.$
Therefore, $\hat{\beta}_{ridge}\sim {\cal N}( (X^TX+\lambda I)^{-1}X^TX\beta,\sigma^2(X^TX+\lambda I)^{-1}X^TX(X^TX+\lambda I)^{-1}).$
\end{proof}

\begin{exercise}
  The two exercises above assumed the residual variance $\sigma^2$ is known. This is unlikely to be the case. Propose a strategy for estimating the standard error of $\hat{\beta}_{LS}$ from data, when $\sigma^2$ is unknown. Implement it in R, and test it on the dataset \texttt{Prestige} in the R package \texttt{cars} (there's a starter script, \texttt{prestige.R} on Github). Do you get the same standard errors as the built-in function \texttt{lm}?
\end{exercise}

\begin{proof}
We can estimate $\sigma^2$ with $\hat{\sigma^2} = \frac{\sum_i(y_i-x_i^T\beta)^2}{n-df}$. We can show that this is an unbiased estimator. Then $se(\hat{\beta}_{LS}) = diag(\sqrt{\frac{\sum_i(y_i-x_i^T\beta)^2}{n-df}(X^TX)^{-1}}).$ The unbiasedness is shown in the following way:
\begin{equation*}
\begin{split}
\E [\hat{\sigma^2}] &= \E[y^T(I-M)y]/(n-df) \\
&= [tr((I-M)Cov(y))+(X\beta)^T(I-M)X\beta]/(n-df)\\
&= \sigma ^2 tr((I-M))/(n-df) = \sigma^2,
\end{split}
\end{equation*}

where $M = X(X^TX)^{-1}X^T.$

\end{proof}

\subsection{Propogation of uncertainty}
Let's now consider the general case where we have a point estimate $\hat{\theta} = (\hat{\theta}_1,\dots, \hat{\theta}_P)^T$ to some set of parameters $\theta = (\theta_1,\dots,\theta_P)^T$, and we have an estimate $\hat{\Sigma}$ to the covariance matrix of the sampling distribution of $\hat{\theta}$. If we want to describe our uncertainty about the individual $\theta_i$ (as was the case for calculating standard errors in the regression problems above), we can look at the diagonal terms in the covariance matrix, $\hat{\Sigma}_{ii} = \hat{\sigma}_i^2$. If we care, more generally, about a \textit{function} of the $\theta_i$, however, the cross terms will become important.

\begin{exercise}
  Let's assume we care about $f(\theta) = \sum_i \theta_i$. What is the standard error of $f(\hat{\theta})$?
\end{exercise}

\begin{proof}
$$sd(f(\hat{\theta})) = \sqrt{var(\sum\theta_i)} = \sqrt{\sum var(\theta_i) + 2\sum_{i<j}cov(\theta_i,\theta_j)}.$$
\end{proof}
\begin{exercise}
  How about the standard error of some arbitrary non-linear function $f(\hat{\theta})$? \textit{Hint: Try a Taylor expansion}
\end{exercise}
\begin{proof}
Suppose $\hat{\theta}$ is close to $\theta$, 
$$sd(f(\hat{\theta})) = \sqrt{var(f(\hat{\theta}))} \approx \sqrt{var(f'(\theta)^T(\hat{\theta}-\theta))} = \sqrt{f'(\theta)^T\hat{\Sigma}f'(\theta)}.$$
\end{proof}

\end{document}